# 机器学习与算法

## （一）常用概念

1. 建模

   利用模型学习已知结果的数据集中的变量特征，并通过一系列方法提高模型的学习能力，最终的对一些结果未知的数据集输出相应的结果。

2. 数据集

   1. 类型
      1. 训练集：训练模型
      2. 验证集：验证模型并对超参数进行选择
      3. 测试集：评估模型
   2. 划分
      1. 留出法
         + 将数据集划分为两个互斥的集合（一般是7:3）
         + 划分时注意保持数据分布的一致性
         + 在使用时一般采用若干次随机划分、重复进行试验评估后取平均值作为结果
      2. k折交叉验证
         + 将数据集划分为k个互斥子集，划分时保持数据分布分布一致性
         + 每次用k-1个子集的并集作为训练集，余下的子集作为测试集，进行k次训练和测试
         + k个测试结果的均值作为最后结果（k一般取10）
         + 在划分k个子集时通常随机划分并重复p次，进行p次k折交叉验证（p一般取10，如10次10折交叉验证）
      3. bootstrap自助法
         + 有放回抽样
         + 初始数据集中约有36.8%的样本未出现在采样数据集中
         + 常用在数据集较小、难以有效划分训练/测试

3. 参数与超参数的区别

   1. 参数

      通过模型对训练集的拟合获得

   2. 超参数

      在训练前需要人为地给出超参数（如决策树模型的深度、随机森林模型中树的数量），只能通过在验证集中进行验证，并进行最终的选择

4. 偏差、方差、误差

   1. 偏差

      反映了模型在训练集样本上的期望输出（预测值）与真实结果（真实值）之间的差距，反映模型本身的拟合能力

   2. 方差

      反映了模型在不同训练集下得到的结果与真实结果之间误差的波动情况，反应模型的稳定性

   3. 误差

      偏差和方差相加

5. 过拟合和欠拟合

   1. 过拟合

      由于训练集中会存在噪声，并且该噪声不具有通用性，不同训练集会有不同的噪声，当模型过于复杂时，会大量学习训练集中的噪声，最终导致模型泛化能力变差

   2. 欠拟合

      偏差过高反映了模型存在欠拟合，表明模型过于简单，没有很好地拟合训练集变量之间的特征，需要进一步提升模型的复杂度

6. 数据预处理时，如何处理类别特征

   1. 序号编码
      + 主要用于处理类别间有大小关系的数据。
      + 序号编码会根据大小关系对类别型特征赋予一个数字ID
      + 例如：成绩可以分为高、中、低三个档次，并且存在‘高>中>低’的排序关系。3表示高、2表示中、1表示低。转化后依旧保留了大小关系。
   2. 独热编码
      + 通常用于处理类别间不存在大小关系的特征。
      + 特征有n个取值，独热编码会把该特征变成一个n维稀疏四维向量。
      + 当类别型特征取值较多时，使用独热编码后需要注意搭配一些特征选择的方法降低维度，这样可以有效避免过拟合问题。
      + 例如：血型一共有四个取值（A型、B型、AB型、O型），独热编码会把血型变成一个四维稀疏向量，说通俗一点就是，把一个特征变成四个特征。例如A型血表示为（1,0,0,0）、B型血表示为（0,1,0,0）、AB型血表示为（0,0,1,0）、O型血表示为（0,0,0,1）。
   3. 二进制编码
      + 先用序号编码给每个类别赋予一个类别ID。
      + 然后将类别ID对应的二进制编码作为结果。
      + 二进制编码本质上利用二进制ID进行哈希映射，最终得到0/1特征向量，且维度数量少于独热编码，节省了存储空间。
      + 以血型为例，A血型的ID为1，二进制表示为001；B血型的ID为2，二进制表示为010；AB型血型的ID为3，二进制表示为011，以此类推可以得到O型血的表示方式。
   4. 其他编码：Helmert Contrast、Sum Contrast、Polynomial Contrast、Backward Difference Contrast
   5. 不编码：使用LGBM(只能处理数值类型的类别型特征）或者CatBoost（可以处理字符串和数值类型的类别型特征）模型并在建模时指定哪些特征是类别型特征，之后模型就可以自动处理它们

7. 二分类问题，正负样本不平衡如何处理

   1. 基于数据的方法

      1. 随机过采样/欠采样

         + 随机过采样（过拟合）

           从少数类样本集随机有放回抽取样本以得到更多的样本

         + 随机欠采样（欠拟合）

           从多数类样本集随机抽取较少的样本

      2. smote过采样

         + 对少数类样本中每个样本，从它在少数类样本的k近邻中随机选择一个样本，然后将这个样本与原样本进行连线，在线上随机选择一点即为新生成的样本（根据需要过采样的倍率重复上述过程若干次）
         + 相比于随机过采样可以降低过拟合的风险
         + 为每个少数类样本合成相同数量的新样本，这可能会增大类间重叠度，并且会生成一些不能提供有效信息的样本

      3. smote过采样改进

         + Borderline-SMOTE：只给处在分类边界的少数类样本合成新样本
         + ADASYNZ：给不同的少数类样本合成不同个数的新样本
         + 使用一些数据清理的方法（基于Tomek Lines）来进一步降低合成样本带来的类间重叠，以得到清晰的类簇，从而更好地训练分类器。

      4. 欠采样改进——Informed Undersampling

         + Easy Ensemble算法：每次从多数类样本中随机抽取一个子集（子集中的样本个数≈少数类样本个数），然后使用这个子集+少数类样本训练一个分类器；重复上述过程若干次，得到多个分类器，最终的分类器结果是这多个分类器结果的融合
         + Balance Cascade算法：级联结构，每一级中从多数类样本中随机抽取子集，用子集+少数类样本训练该级的分类器；然后将多数类样本中能够被当前分类器正确判别的样本剔除掉，继续下一级的操作，重复若干次得到级联结构；最终的输出结果也是各分类器结果的融合。
         + NearMiss：利用K近邻信息挑选具有代表性的样本
         + One-sided Selection：采用数据清理技术

   2. 基于算法的方法

      + 在样本不均衡时，也可以通过改变模型训练时的目标函数(如代价敏感学习中不同类别有不同的权重）来矫正这种不平衡性。
      + 当样本数目极其不均衡时，也可以将问题转化为单类学习（one-class learning）、异常检测（anomaly detection）。

8. 什么是常用的基分类器？

   最常用的基分类器是决策树，主要有以下三个原因：

   + 决策树可以比较方便地将样本权重整合到模型训练中，而不需要使用过采样的方法调整样本权重。

   + 决策树的表达能力和泛化能力，可以通过调节树的层数来做折中。

   + 数据样本对决策树的扰动对于决策树的影响较大，因此不同子样本集合生成的决策树基分类器随机性较大，这样‘不稳定的学习器’更适合做基分类器。此外决策树在节点分裂的时候，随机地选择一个特征子集，从中找出最优分裂属性，很好地引入了随机性。

   除了决策树外，神经网络模型也适合作为基分类器，主要由于神经网络模型也比较“不稳定”，而且还可以通过调整神经元数量、连接方式、网络层数、初始权值等方式引入随机性。

## （二）常见模型分类方法

1. 监督学习和无监督学习

   1. 监督学习：训练数据既有特征又有标签

      1. 预测问题：标签连续

         线性回归模型、时间序列模型、神经网络模型

      2. 分类问题：标签不连续

         逻辑回归模型，SVM模型、决策树模型、随机森林模型、Bootsing模型

   2. 无监督学习：训练数据只有特征没有标签

      1. 聚类问题

         K-Means聚类

      2. 降维问题

         主成分分析

2. 参数模型和非参数模型

   1. 参数模型：
      + 目标函数形式在训练前已经确定
      + 包括线性回归模型、逻辑回归模型、朴素贝叶斯模型
      + 具有很强的可解释性，模型学习和训练相对快速，对数据量要求比较低
      + 需要对提前对目标函数作出假设，针对复杂问题容易产生欠拟合
   2. 非参数模型
      + 在训练前目标函数形式没有限定，通过训练不断修改目标函数的形式
      + 常见的有SVM模型、决策树模型、随机森林模型
      + 对目标函数的形式不做过多的假设，当数据量趋向无穷大时，可以逼近任何复杂的真实模型
      + 与参数模型相比更复杂，计算量更大，对问题的可解释性更弱

   3. 半参数模型——神经网络模型
      + 如果固定了隐层的数目以及每一层神经元的个数，也属于参数模型
      + 但由于隐层数目与每一层神经元的个数在模型训练中通常是不固定的，需要通过验证集进行选择，因此属于半参数模型

3. 生成模型和判别模型

   1. 生成模型
      + 通过学习得到联合概率分布P(x,y)，即特征x和标签y共同出现的概率，然后求条件概率分布，能够学习到数据生成的机制
      + 常见的有朴素贝叶斯模型、混合高斯模型、隐马尔科夫模型
      + 需要的数据量比较大，能够较好地估计概率密度
      + 在数据量充足的情况下，生成模型的收敛速度比较快，并且能够处理隐变量问题
      + 相比于判别模型，生成模型需要更大的计算量，准确率及适用范围也弱于判别模型
   2. 判别模型
      + 通过学习得到条件概率分布P(x|y)，即在特征x出现的情况下标签y出现的概率
      + 常见的有决策树模型、SVM模型、逻辑回归模型
      + 对数据量的要求不高，更常用

## （三）常见模型

1. 线性回归模型

   确定两个或两个以上变量间相互依赖的定量关系的一种统计分析方法

   1. 假设

      随机误差项零均值（条件均值）、同方差（条件方差）、无序列自相关、与解释变量不相关、正态性（条件分布），各解释变量间无多重共线性

   2. 提升方法

      1. 随机误差项不满足正态性或方差齐性

         考虑对模型进行改进（如：对因变量采用log对数转化再进行回归）

      2. 多重共线性

         - 采用ridge回归（岭回归）：在损失函数的基础上加上对参数θ的L2惩罚项，使最小二乘法求解参数θ时有解。同时将θ尽量压缩到0。
         - 采用逐步回归法筛选特征变量：原理是对每个解释变量依次引入模型进行F检验，同时对已引入的解释变量逐个进行T检验，当原来引入变量由于后面加入的变量的引入而不再显著变化时，则剔除此变量，确保每次引入新的变量之前回归方程中只包含显著性变量，直到既没有显著的解释变量选入回归方程，也没有不显著的解释变量从回归方程中剔除为止，最终得到一个最优的变量集合。
         - 可以考虑做PCA降维提取新的解释变量：所提取的变量间的组内差异小而组间差异大，起到了消除共线性的问题。
         - 删除不重要的解释变量x：自变量之间存在共线性，说明自变量所提供的信息是重叠的，可以删除不重要的自变量减少重复信息。

   3. 优缺点

      1. 优点

         快速并且可解释性强，可以有效指导业务部门决策

      2. 缺点

         无法处理复杂问题
         
         

2. 逻辑回归模型

   1. 定义

      + 逻辑回归是线性回归通过S函数将结果映射到0，1区间内
      + 做分类判断时，设定一个分类的阈值，大于阈值则样本预测值为1，小于阈值则样本预测为0

   2. 原理

      假设数据服从伯努利分布,通过极大化似然函数的方法，运用梯度下降来求解参数，从而实现数据二分类

   3. 正则化方法中L1与L2方法的区别

      通过引入惩罚项，使得逻辑回归模型中各个变量的系数得以收缩，从而避免过拟合的发生

      1. L1方法——lasso方法
         + 惩罚系数的绝对值，惩罚后有的系数直接变成0，其他系数绝对值收缩
         + 可以筛选变量，在变量较多的情况下，能够从中选择较为重要的变量

      2. L2方法——ridge方法
         + 惩罚系数的平方，惩罚后每个系数的绝对值收缩

   4. 逻辑回归函数

      1. 二分类——sigmoid函数
         $$
         S(x) = \frac{1}{1+e^{-x}}
         $$
         
      2. 多分类——softmax函数

         可以针对多个分类输出概率值，概率值之和为1

   5. 优缺点

      优点

      + 模型输出即为样本的概率分布；
      + 能够输出表达式并且分析各个特征的权重；
      + 在SPSS,Python中有许多现成的包能够调用，并且构建的模型可解释性高。

      缺点

      + 预测结果呈S型分布，两端的概率变化非常小，中间的概率变化十分剧烈，难以找到阈值；
      + 不引入其他方法的情况下，只能处理线性可分的数据；
      + 拟合能力差；
      + 很难处理数据不平衡问题。

   6. 常用场景

      广告点击行为预测；借款人评估；信用评分卡

   7. 逻辑回归和线性回归的区别和联系
      1. 区别
         - 线性回归假设因变量服从正态分布，逻辑回归假设因变量服从伯努利分布。
         - 线性回归的目标函数是均方误差，逻辑回归的目标函数是交叉熵。
         - 线性回归要求自变量与因变量呈线性关系，逻辑回归无要求。
         - 线性回归是处理回归问题，逻辑回归是处理分类问题。
      2. 联系
         - 两个模型都是线性模型，线性回归是普通线性模型，逻辑回归是广义线性模型。
         - 逻辑回归是线性回归通过sigmod函数将结果映射到[0,1]区间内，并设定一个阈值，大于这个阈值则被预测为1，小于阈值则被预测为0。
   8. 牛顿法和梯度下降法的区别
      1. 牛顿法
         + 是一种在实数域和复数域上近似求解方程的方法
         + 方法使用函数f (x)的二阶泰勒展开来寻找方程f (x) = 0的根
         + 牛顿法最大的特点就在于它的收敛速度很快，但牛顿法需要计算海森矩阵（多元函数的二阶导数构成的矩阵）的逆，计算复杂，可以起到减小步长的效果
         + 一般采用拟牛顿法解决牛顿法复杂计算的问题，拟牛顿法只需要用到一阶导数，不需要计算海森矩阵的逆，而采用正定矩阵来近似Hessian矩阵的逆，简化运算的复杂度。
      2. 梯度下降法
         + 通过搜索方向和步长来对参数进行更新
         + 其中搜索方向是目标函数在当前位置的负梯度方向。因为这个方向是最快的下降方向
         + 步长确定了沿着这个搜索方向下降的大小。梯度下降法中的步长是一定的，因此越接近最优值时，步长应该不断减小，否则会在最优值附近来回震荡。

3. 线性判别分析LDA

   1. 定义

      + 是一种线性的分类模型（可以多分类），它的目标是建立一条直线，让全体训练样本投影到这条直线上，使得同类样本的方差尽可能小，异类样本的中心尽可能远，从而实现分类的目的。
      + 同时它也可以做有监督的数据降维。这个算法主要是用于降维比较多一点。

   2. 假设

      - 数据集服从正态分布。
      - 特征之间相互独立。
      - 每个类的协方差矩阵相同。

   3. 优缺点

      优点

      - 在降维过程中可以使用类别的先验知识经验，而像PCA这样的无监督学习则无法使用类别先验知识。
      - LDA在样本分类依赖的是均值而不是方差时，比PCA之类的算法较优。

      缺点

      - LDA不适合对非高斯分布的样本降维。
      - LDA降维最多降到类别数k-1的维数，如果我们降维的维度大于k-1，则不能使用LDA。目前也有一些LDA改进的方法可以解决这个问题。
      - LDA在样本分类信息依赖方差而不是均值的时候，降维效果不好。
      - LDA可能过度拟合数据。

   4. PCA和LDA的区别

      PCA（主成分分析）和LDA（线性判别分析）有很多的相似点，其本质是要将初始样本映射到维度更低的样本空间中，但是PCA和LDA的映射目标不一样：PCA是为了让映射后的样本具有最大的发散性；而LDA是为了让映射后的样本有最好的分类性能。所以说PCA是一种无监督的降维方法，而LDA是一种有监督的降维方法。

      

4. KNN

   1. 定义

      K近邻算法是一种基本的回归和分类算法。K近邻算法多用于对数据集合中每一个样本进行分类，如果有一个样本，附近有K个样本，这K个样本中大多数属于某个类别，那么这个样本就属于这个类别，和随机森林的多数投票类似。

   2. 过程

      1. 计算已知类别数据集中的点与当前点之间的距离。
      2. 按距离递增次序排序。
      3. 选取与当前点距离最小的k个点。
      4. 统计前k个点所在的类别出现的频率。
      5. 返回前k个点出现频率最高的类别作为当前点的预测分类。

   3. 三要素

      1. k值的选择：k值的选择会对结果产生重大影响。
         + 较小的k值可以减少近似误差，但是会增加估计误差（过拟合）；
         + 较大的k值可以减小估计误差，但是会增加近似误差（欠拟合）。
         + 一般而言，通常采用交叉验证法来选取最优的k值，k一般为奇数。

      2. 距离度量：距离反映了特征空间中两个实例的相似程度。可以采用欧氏距离、曼哈顿距离等。

      3. 决策规则：在分类模型中，主要使用多数表决法或者加权多数表决法；在回归模型中，主要使用平均值法或者加权平均值法。

         + 多数表决：邻近的K个样本，每个样本权重一样，也就是说最终预测的结果为出现类别最多的那个类。

         + 加权多数表决：邻近的K个样本中，距离近的样本相较与远的样本权重高。

   4. 优缺点

      优点

      + 既可以用来做分类也可以用来做回归
      + 适用于样本容量大的自动分类
      + 对异常值不敏感

      缺点

      + 预测速度相较于逻辑回归慢
      + 输出可解释性不强
      + 样本不平衡时效果不好
      + 计算量较大

   5. 应用场景

      文本自动分类

      

5. 朴素贝叶斯

   1. 定义

      + 朴素贝叶斯是贝叶斯分类算法中的一种，以贝叶斯定理为基础，对数据进行分类
      + 根据贝叶斯公式，计算实例属于各个类的后验概率，并将实例分类到概率最大的类中

   2. 过程

      1. 训练分类器，计算输入的训练样本中，每个不同的分类类别在训练样本中出现的频率以及每个类别下各个特征属性出现的条件概率值
      2. 对于待分类的样本，计算在该样本出现的条件下，各个分类类别出现的概率，哪个概率最大，待分类的样本就属于哪个类别。

   3. 优缺点

      优点

      + 结果可解释性高
      + 对缺失值不敏感
      + 有稳定的分类效率
      + 小规模数据表现良好

      缺点

      + 特征之间相关性高的时候效果不好
      + 分类决策存在错误率
      + 对输入数据的表达形式很敏感

   4. 应用场景

      文本分类、情感检测、拼写检测

6. SVM模型

   SVM是在特征空间上找到最佳的分离超平面，使得训练集上的正负样本间隔最大。是用来解决二分类问题的有监督学习算法，在引入核方法后也可以解决非线性问题。

   

7. 决策树模型

   1. 定义

      + 一种非参数模型，无需对目标函数和变量做过多的假设，使用灵活，能够处理复杂问题
      + 一种树形结构，决策树主要包含3种结点（根结点：初始结点；叶节点：最终分类结果结点；内结点：树内部进行判断的条件结点-即特征） 在决策树中，每个样本都只能被一条路径覆盖。

   2. 基本思想

      以信息熵为度量构造一棵熵值下降最快的树。到叶子节点处的熵值为零，此时每个叶结点中的实例都属于同一类。

   3. 优缺点

      1. 优点

         + 易于理解和解释、可以可视化分析，容易提取出规则

         + 速度快、计算量相对较小

         + 可以同时处理分类问题和预测问题

         + 对缺失值不敏感

      2. 缺点
         + 数据集特征很多时，容易过拟合
         + 忽略了特征之间的相关性

   4. 信息熵
      $$
      H(x) = -\sum_{i=1}^{n}{p(x_i)}log{p(x_i)}
      $$

      - 状态取值越多，信息熵越大
      - 每个状态取值的概率越平均，信息熵越大，反之各状态的概率越是相差悬殊，信息熵就越小。
      - 信息熵实际上衡量随机事件的不确定程度，越是不确定，信息熵就越大，那用来表达清楚这条信息所需要的比特数就越多。

   5. 信息增益
      $$
      g(D,A) = H(D) - H(D|A)
      $$
      

      + 得知特征A的信息而使D的信息的不确定性减少的程度。
      + 使用划分前后集合熵的差值来衡量使用当前特征对于样本集合划分效果的好坏。 
      + 信息增益=使用该特征分类前的信息熵– 使用该特征分类后的信息熵。

   6. 信息增益比

      信息增益比=信息增益/特征熵
      $$
      g_{R}(D,A) = \frac{g(D,A)}{H_{A}(D)} = \frac{ H(D) - H(D|A)}{H_{A}(D)},\\
      H_{A}{D} = -\sum_{i=1}^{n}{\frac{|D_{i}|}{|D|}log_{2}{\frac{|D_{i}|}{|D|}}}
      $$
      |Di|/|D|表示样本在节点各个分类数量上的占比

   7. 基尼指数
      $$
      Gini(D) = 1 - \sum_{i=1}^{n}{p_{i}^{2}}
      $$

      + 表示在样本集合中一个随机选中的样本被分错的概率。
      + 基尼指数（基尼不纯度）= 样本被选中的概率 * 样本被分错的概率。
      + 假设一共有n个类别，pi代表被选中的样本属于第i类别的概率，基尼指数越小代表纯度越高，当类别只有1个的时候，基尼指数为0。

   8. 确定节点特征的常用方法

      1. ID3

         使用信息增益进行特征选择。当特征的取值较多时，根据此特征划分更容易得到纯度更高的子集，因此划分之后的熵更低，由于划分前的熵是一定的，因此信息增益比较偏向取值较多的特征。

         

         优点

         + 处理离散型数据
         + 倾向于选择取值较多的属性，因为信息增益反映的给定一个条件以后不确定性减少的程度,必然是分得越细的数据集确定性更高,也就是条件熵越小,信息增益越大。

         缺点

         + 不能处理连续型数据和带有缺失值的数据集。
         + 如果出现各属性值取值数分布偏差大的情况, 分类精度大幅度降低。
         + 没有考虑过拟合问题。

      2. C4.5

      ​                定义

      + 使用信息增益比进行特征选择，在选择特征时，会选择能够使信息增益比最大化的特征作为节点；

        + 随着分枝数量的增加，信息增益比会相应的变小，可以避免选择有过多取值的特征作为节点；
        + 可以使用weka库实现（sklearn只有ID3和CART）

        

        优点

        + 克服了信息增益选择特征的时候偏向于特征取值个数较多的不足
        + 可以处理连续性数据和缺失值
        + 加入了树的后剪枝，有效解决决策树过拟合问题

        缺点

        + C4.5和ID3一样，使用的模型算法含有大量的对数计算，增加了模型的时间复杂度
        + 相比于ID3，加入了树的后剪枝，因此，需要对数据进行多次扫描,算法效率低
        + 不能处理回归问题
        + 和ID3一样采用的是多叉树运算，相比于二叉树运算效率低。

        

      3. CART

         定义

         + 分类回归树，既可以用于分类，也可以用于预测；
         + 分类模型使用基尼系数进行特征选择，回归模型采用和方差划分特征；
         + 预测方式上，CART分类树采用叶子节点里概率最大的类别作为当前节点的预测类别。回归树输出不是类别，采用叶子节点的均值或者中位数来预测输出结果。（cart是可以自动处理缺失值的，但是sklearn中的cart不能自动处理缺失值）

         优点

         + 既适用于回归，也可以用于预测
         + 采用基尼指数选择特征，相比于信息增益，计算速度更快，因为它不用计算log值
         + CART 构建回归树用到树的剪枝技术，用于防止树的过拟合

         缺点

         + 每次用最优特征进行划分，这种贪心算法很容易陷入局部最优，事实上分类决策不应该由某一特征决定，而是一组特征决定的（有一种多变量决策树可以解决，它会选择一个最优的特征线性组合做决策，代表算法OC1）
         + 样本一点点改动，树结构剧烈改变，可以通过集成算法解决

   9. 常用的调优方法

      + 控制树的深度和节点个数，避免过拟合
      + 运用交叉验证的方法，选择合适的参数
      + 通过模型集成的方法，基于决策树形成更复杂的模型

   10. 决策树防止过拟合

       决策树通过剪枝防止过拟合，处理由于数据中的噪声和离群点导致的过分拟合问题

       剪枝:给决策树瘦身，也就是不需要太多判断就可以得到好的结果，防止过拟合发生。包括预剪枝和后剪枝。

       1. 预剪枝

          在构造过程中，当某个节点满足剪枝条件，则直接停止此分支的构造。

          + 当树达到一定深度的时候，停止树的生长。
          + 当到达当前结点的样本数量小于某个阈值的时候，停止树的生长。
          + 计算每次分裂对测试集的准确度提升，当小于某个阈值时，不再继续拓展。

       2. 后剪枝

         先构造完成完整的决策树，然后自底向上的对非叶结点进行考察，若将该结点对应的子树换为叶结点能够带来泛化性能的提升，则把该子树替换为叶结点。部分算法自带。

   11. 决策树停止分裂的条件

       - 低于结点划分所需要的最小样本数，也就是超参数min_samples_split，还有一个比较相似的超参数是叶子结点所需的最小样本数min_sample_leaf。为什么它们可以作为停止分裂的条件，是因为当某个结点样本数已经很少了，如果再继续划分，就可能会使模型达到过拟合，也可能扩大噪声（错误）数据的影响。
       - 达到树的最大深度，对应超参数max_depth。如果不设置这个超参数的话，那么决策树会默认不断地划分下去，直到达到叶子结点的基尼指数为0或者熵值为0停止分裂。很明显，会造成树的过拟合。
       - 结点的基尼指数或者熵值小于所设定的阈值，对应超参数min_impurity_split，这个也很容易理解，因为基尼指数或者熵值越小，代表不确定越低，说明目前的纯度已经很高了，就没有必要再划分下去了。
       - 所有的特征已经划分完毕了，自然就停止了。

   12. 决策树可视化

       ```Python
       from sklearn.model_selection import train_test_split
       
       from sklearn.tree import DecisionTreeClassifier
       
       from sklearn import datasets
       
       import matplotlib.pyplot as plt# 加载鸢尾花数据
       
       iris=datasets.load_iris()
       
       X=iris['data']
       
       y=iris['target']# 查看鸢尾花类别的名字
       
       feature_names=iris.feature_names# 将数据分为训练数据和测试数据，比例是4:1
       
       X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=1024)# 训练数据，预测鸢尾花的种类
       
       clf=DecisionTreeClassifier(criterion=' gini')
       
       clf.fit(X_train,y_train)
       
       y_=clf.predict(X_test)
       
       plt.figure(figsize=(18,12))# filled填充color
       
       _=tree.plot_tree(clf,filled=True,feature_names=feature_names) #下图petal length(cm)为特征名称 gini为基尼指数 samples为样本数 value是各个类别的样本数（一共有三个类别，四个特征（sepal（萼片长度和宽度），petal（花瓣长度和宽度））
       ```

       

8. 随机森林

   1. 弱学习器和强学习器

        + 没有明确的划分界限，是一个相对概念，体现在学习器对复杂数据场景的处理能力上
        + 通过集成方法，可以将多个弱学习器构造成一个强学习器

   2. 模型集成和模型融合

        1. 模型集成

           将多个弱学习器（基模型）进行组合，以提高模型的学习泛化能力

           + 同质集成模型：将相同种类模型进行集成的模型
           + 异质集成模型：将不同种类模型进行集成的模型

           常见的有Bagging和Boosting

        2. 模型融合

           在模型集成中，需要将各个基模型的结果进行组合，得到最终的结果，称为模型融合

           + 平均法：在预测问题中，将各个基模型的结果进行平均作为最终结果
           + 投票法：在分类问题中，选择基模型中预测比较多的类别作为最终结果

   3. 随机森林

        1. 定义

           随机森林由很多决策树构成，当我们输入一个要预测分类的数据时，每个决策树都会接收数据并产生一个分类结果，然后采用投票机制，认为哪个结果出现的次数最多就归为哪一类，做回归时采用平均值计算作为预测结果

        2. 基本原理

           - 通过对样本或变量的n次随机采样，可以得到n个样本集
           - 对于每一个样本集，可以通过独立训练决策树模型
           - n个决策树模型之间是相互独立的，并不是完全独立，训练集之间有交集
           - 对于n个决策树模型的结果，通过集合策略来得到最终输出
           - 可通过Bootstrap方法实现样本的随机采样

        3. 过程

           ```mermaid
           graph LR
           A[随机样本选取]-->B[随机选取特征]
           B-->C[构建决策树]
           C-->D[随机森林投票]
           ```

        4. 优点

           + 有效解决决策树的过拟合问题，能够处理特征维度很多的数据，并且不需要做特征选择，可以解决回归或者分类问题
           + 容易理解和解释，树可以被可视化
           + 不需要进行数据归一化

        5. 缺点

           + 不适合小样本，只适合大样本。
           + 会在某些噪声较大的分类或回归问题上会过拟合

        6. 应用场景

           预测贷款风险

   4. 随机森立模型对决策树模型的优势

        + 假设各个决策树模型有相同的偏差和方差，通过多个决策树模型得到的结果进行平均或投票，可以保证随机森林模型的偏差与单个决策树模型的偏差基本相同

        + 由于各个决策树模型之间的相互独立性，通过对结果进行平均或者加权能够大幅度减小随机森林模型的方差，最终将误差变小

          

9. Boosting模型

     将多个决策树模型集成后的一种模型

     1. 与随机森林模型的区别

        1. 随机森林采用Bagging方法，各个决策树模型的生成是相互独立的，是基于通过样本重采样方法得到的不同训练集而产生不同的决策树模型
        2. Boosting模型中新的决策树模型时基于此前已经生成的决策树模型的结果，所以决策树模型的生成不是相互独立的，每一个新的决策树模型都依赖前一个决策树模型

     2. 常见的Boosting方法及原理

        1. AdaBoost

           该方法会加大此前决策树模型中分类错误的数据的权重，是的下一个生成的决策树模型能够尽量将这些训练集分类正确

        2. GBDT

           通过计算损失函数梯度下降方向，定位模型的不足而建立新的决策树模型，用途更为广泛

     3. 随机森林和GBDT的优缺点
        1. 优点
           + 都是基于决策树模型，能够处理离散型变量和连续性变量同时存在的场景
           + 不需要对数据集做过多的假设，能够处理比较复杂的问题
           + 都属于集成学习方法，相比单一的决策树模型，性能有了很大的提升
        2. 缺点
           + 面对更大的训练集时，依然存在速度较慢的问题

10. GBDT模型——梯度提升决策树

      1. 原理

         + 利用boosting集成算法的思想，按顺序生成多个弱学习器，每次都是通过拟合上一次误差函数对预测值的残差建立一个新的学习器，所有弱学习器的结果相加等于最终的预测值。
         + 虽然GBDT可以做回归也可以做分类，但是它用到的所有树都是cart回归树，而不是cart分类树，因为分类树的结果是没办法累加的。
         + 在特征选择上，回归树在分枝时会穷举每一个特征的每个阈值以找到最好的分割点，衡量标准是最小化均方误差。

      2. 正则化

         + 定义步长（learning rate）：就是给每一颗树的预测结果乘以一个步长，减少之前树的影响，有利于后续创建的树有更大的学习机会。
         + 子采样比例（subsample）：和随机森林的子采样不同，gbdt的子采样是对样本进行无放回抽样。将采样比例设置在0.5-1之间可以起到减少方差，减低过拟合的作用。
         + 对弱学习器cart回归树进行剪枝：如控制树的最大深度、节点的最少样本数、最大叶子节点数、节点分支的最小样本数等。

      3. 优缺点

         优点

         + 能够很好地处理线性及非线性数据
         + 在相对少的调参时间情况下，预测的准确率也可以比较高。这个是相对SVM来说的。
         + 使用一些健壮的损失函数，对异常值的鲁棒性非常强。比如 Huber损失函数和Quantile损失函数。

         缺点

         + 由于弱学习器之间存在依赖关系，难以并行训练数据。
         + 对异常值敏感。

      4. 应用

         在回归、分类以及多分类问题上，都很适用。

11. XGBoost模型

        1. 原理
           + 不断地添加树，然后进行特征分裂，每次添加一个树，就是学习一个新函数，拟合上一次预测的残差。
           + 当训练完成得到k棵树，我们要预测一个样本的分数，其实就是根据这个样本的特征，在每棵树中会落到对应的一个叶子节点，每个叶子节点就对应一个分数
           + 最后只需要将每棵树对应的分数加起来就是该样本的预测值。
        
        2. 与GBDT的区别
        
           GBDT是机器学习算法，XGBoost除了算法内容还包括一些工程实现方面的优化。 
        
           - 基于二阶导：GBDT使用的是损失函数一阶导数，相当于函数空间中的梯度下降；而XGBoost还使用了损失函数二阶导数，相当于函数空间中的牛顿法。 
           - 正则化：XGBoost显式地加入了正则项来控制模型的复杂度，能有效防止过拟合。 
           - 列采样：XGBoost采用了随机森林中的做法，每次节点分裂前进行列随机采样，不仅可以防止过拟合，还能减少计算量。 
           - 缺失值处理：XGBoost运用稀疏感知策略处理缺失值，而GBDT没有设计缺失策略。 
           - 并行高效：XGBoost的列块设计能有效支持并行运算，提高效率。
        
        3. XGBoost的并行操作
        
           + 不是指在模型上并行，而是在特征上并行
        
           + 在训练之前，预先对数据排序，然后保存为块结构，在后面的迭代中会重复使用该结构，大大减少计算量
        
           + 在进行节点选择时，需要计算每个特征的增益，该计算就是基于块结构实现并行操作
        
        4. 预排序算法
        
           + 对所有特征按照特征的数值进行排序，然后遍历每个特征上的所有分裂点，计算该分裂点分裂后的全部样本的目标函数增益，找到最大增益对应的特征和候选分裂点位，从而进行分裂，一层一层的完成建树过程。
           + 需要额外空间存储预排序的结果，还会造成时间上很大的开销，每遍历一个分割点都需要计算分裂增益，并且在选择最大增益的时候需要对比所有样本的增益情况。
        
        5. XGB为什么要把全部样本特征值算出来，放到内存中去
        
           + XGB关于最优特征以及最优切分点的选取采用的是一种贪心算法，是基于某种标准（GINI系数/均方误差/信息增益比等），需要依次遍历所有特征和所有特征的取值，通过线性扫描来找到最优分割点，这是需要将所有数据同时读入内存。
           + 当数据量过大时，这就不适用了。于是xgb也提供了另一种近似直方图算法，相比于贪心算法需要线性扫描特征的每个分裂点，而近似直方图算法只考察分位点，将连续的特征映射到对应的桶中，然后遍历桶之间的间隙来找最佳的分裂分位点，这样可以有效的减少计算量。
        
        6. XGB的正则化项
        
        7. 优缺点
        
           优点
        
           - 精度更高：采用了二阶泰勒展开，保留了目标函数更多的信息。
           - 正则化：引入L1L2正则化，降低模型过拟合。
           - 灵活性强：不仅支持cart作为基分类器还支持线性分类器。
           - Shrinkage（学习率缩减）：每次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。
           - 支持列抽样：降低过拟合，还能减少计算。
           - 自动处理缺失值：采用的稀疏感知算法可以自动学习出它的分裂方向。
           - 支持特征并行：减少计算量，降低耗时。
           - 近似直方图算法处理结点分裂：更加高效地划分特征。
        
           缺点
        
           - 结点分裂过程中需要遍历所有数据集，耗时长。
           - 预排序过程的空间复杂度过高，导致消耗的内存多。
           - 只能够处理连续型的数值。
           - 需要调整的超参数比较多。
        
        8. XGB调参
        
           常用调参方式有网格搜索、随机搜索、贝叶斯优化。超参数调优顺序如下：
        
           1. 确定学习速率leaning_rate和确定参数调优的初始值以及评估指标
              + 学习率刚开始通常设置一个比较大的数便于提升模型训练速度
              + 回归可以以R2作为评估指标，分类可以使用F1或者AUC作为评估指标。
              + 初始取值是自己决定的，和算法自带的默认取值不完全相同
           2. 调整n_estimators（树的数量）
              + 取值范围可以是[400, 500, 600, 700, 800]
              + 树的数量越多越容易过拟合
           3. 调整min_child_weight（最小叶子节点样本权重和）和max_depth（最大深度）
              + max_depth取值范围[3, 4, 5, 6, 7, 8, 9, 10]
              +  min_child_weight取值范围[1, 2, 3, 4, 5, 6]
              + min_child_weight越大容易欠拟合，max_depth越大容易过拟合。
           4. 调整gamma（惩罚系数：节点分裂所需的最小损失函数下降值）
              + 取值范围是[0.1, 0.2, 0.3, 0.4, 0.5, 0.6]，
              + gamma越大越不容易过拟合。但性能就不一定能保证,需要平衡。
           5. 调整subsample（随机选样本的比例）以及colsample_bytree（随机选取特征的比例）
              + 'subsample'取值范围是 [0.6, 0.7, 0.8, 0.9], 
              + 'colsample_bytree'取值范围是 [0.6, 0.7, 0.8, 0.9]
              + 两者都是越大越容易过拟合，
        
           6. 调整reg_alpha（L1正则）以及reg_lambda（L2正则）
              + 取值范围均为 [0.05, 0.1, 1, 2, 3]都是越大越容易欠拟合。
              + L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0，实践中L2正则化通常优于L1正则化。
        
           7. 调整学习率learning_rate
              + 取值范围[0.01, 0.05, 0.07, 0.1, 0.2]
              + 学习率过大越容易过拟合。
        
        9. XGB为什么把全部样本特征值算出来，放到内存中去？
        
           + 因为XGB关于最优特征以及最优切分点的选取采用的是一种贪心算法，是基于某种标准（GINI系数/均方误差/信息增益比等），需要依次遍历所有特征和所有特征的取值，通过线性扫描来找到最优分割点，这是需要将所有数据同时读入内存。
           + 当数据量过大时，这就不适用了。于是xgb也提供了另一种近似直方图算法，相比于贪心算法需要线性扫描特征的每个分裂点，而近似直方图算法只考察分位点，将连续的特征映射到对应的桶中，然后遍历桶之间的间隙来找最佳的分裂分位点，这样可以有效的减少计算量。
        
        10. xgb的正则化项是如何构造的？
        
            xgb的正则化项也是树的复杂度函数，包含两个部分：树里面叶子结点的个数T、树里面叶子结点权重ω的L2正则。
            $$
            \Omega(f) = \gamma T + \frac{1}{2} \lambda \sum_{j=1}^{T}{\omega_j^2}
            $$
        
            + T: 是对叶子的个数加了限制，从而限制了模型的复杂度
        
            + 叶子结点权重ω：通过L2正则是不希望有太大的权重ω出现
        
            + γ：是gamma（结点分裂所需的最小损失函数下降值）
        
            + λ：是L2正则化系数reg_lambda
        
            这里出现了γ和λ，这是xgboost自己定义的，在使用xgboost时，可以设定它们的值，显然，γ越大，表示越希望获得结构简单的树，因为此时对较多叶子节点的树的惩罚越大。λ越大也是越希望获得结构简单的树。
        
        11. XGB怎么给特征评分？
        
            XGB的特征重要性计算，有5个评价指标。
        
            + weight（默认）：在所有树中，特征用作分裂点的总次数。
        
            + total_gain：在所有树中，该特征用作分裂节点带来的gain值之和。
        
            + total_cover：在所有树中，该特征用作分裂节点时，对于样本的覆盖程度之和。简单一点来说，覆盖是指一个特征作为分裂结点参与了某个样本的划分，那么就相当于是这个特征覆盖了这个样本，total_cover指的就是某个特征覆盖了的样本数量。
        
            + gain：计算公式为gain=total_gain/weight，在所有树中，该特征用作分裂节点带来的平均gain值。
        
            + cover：计算公式为，在所有树中cover=total_cover/weight，该特征用作分裂节点时，对于样本的平均覆盖程度。

12. LGBM模型

      1. 定义

         + LightGBM是基于XGBoost的改进版，在处理样本量大、特征纬度高的数据时，XGBoost效率和可扩展性也不够理想，因为其在对树节点分裂时，需要扫描每一个特征的每一个特征值来寻找最优切分点，耗时较大。
         + LightGBM则提出了GOSS（Gradient-based One-Side Sampling，基于梯度的单边采样）和EFB（Exclusive Feature Bundling，互斥特征捆绑）来分别进行样本采样和降低特征维度。同时，使用Histogram来加速寻找切分点。
         + 因此可以简单点说，LightGBM = XGBoost + Histogram + GOSS + EFB。

      2. Histogram直方图算法

         1. 基本思想

            + 先把连续的浮点特征值离散化成K个整数，同时构造一个宽度为K的直方图。
            + 在遍历数据的时候，根据离散化后的值作为索引在直方图中累积统计量。
            + 当遍历一次数据后，直方图累积了需要的统计量，然后根据直方图的离散值，遍历寻找最优的分割点。

         2. 相比预排序算法的优势

            + 很大程度上减少了内存占用和计算代价。

            + 虽然直方图算法找到的分割点并不算是最优的分割点，毕竟相比 xgb 算法来说通过 bins 分箱之后分裂点少了很多。但是正因为这个导致加上了一个正则化的效果，使得生成的弱分类器的有效的防止过拟合。

            + 特征被离散化后，找到的并不是很精确的分割点，所以会对结果产生影响。但在不同的数据集上的结果表明，离散化的分割点对最终的精度影响并不是很大，甚至有时候会更好一点。原因是决策树本来就是弱模型，分割点是不是精确并不是太重要；较粗的分割点也有正则化的效果，可以有效地防止过拟合；即使单棵树的训练误差比精确分割的算法稍大，但在梯度提升（Gradient Boosting）的框架下没有太大的影响。

            + 预排序算法中，根据特征的不同取值进行分割，在直方图算法不考虑特征的取值，直接根据样本的在每个 bin 中的数量，并且在新的 bin 中存储的是特征取值的个数。

              这样在遍历该特征的时候，只需要根据直方图的离散值，遍历寻找最优的分割点即可。并且由于 bins 的数量是远远小于特征不同取值的数量，所有分桶之后要遍历的分裂点个数会少很多，进一步减少计算量。

         3. 直方图做差加速

            + 一个叶子的直方图可以由它的父亲节点的直方图与它兄弟的直方图做差得到，在速度上可以提升一倍。
            + 通常构造直方图时，需要遍历该叶子上的所有数据，但直方图做差仅需遍历直方图的k个桶。
            + 在实际构建树的过程中，LightGBM还可以先计算直方图小的叶子节点，然后利用直方图做差来获得直方图大的叶子节点，这样就可以用非常微小的代价得到它兄弟叶子的直方图。

         4. 带有深度限制的按叶子生长 (leaf-wise) 算法

            + XGBoost 采用 Level-wise 的增长策略，该策略遍历一次数据可以同时分裂同一层的叶子，容易进行多线程优化，也好控制模型复杂度，不容易过拟合。
            + 但实际上Level-wise是一种低效的算法，因为它不加区分的对待同一层的叶子，实际上很多叶子的分裂增益较低，没必要进行搜索和分裂，因此带来了很多没必要的计算开销。
            + LightGBM采用Leaf-wise的增长策略，该策略每次从当前所有叶子中，找到分裂增益最大的一个叶子，然后分裂，如此循环。因此同Level-wise相比，Leaf-wise的优点是：在分裂次数相同的情况下，Leaf-wise可以降低更多的误差，得到更好的精度；Leaf-wise的缺点是：可能会长出比较深的决策树，产生过拟合。因此LightGBM会在Leaf-wise之上增加了一个最大深度的限制，在保证高效率的同时防止过拟合。

         5. 单边梯度采样算法GOSS

            + GOSS算法从减少样本的角度出发，排除大部分小梯度的样本，仅用剩下的样本计算信息增益，它是一种在减少数据量和保证精度上平衡的算法。
            + 根据计算信息增益的定义，梯度大的样本对信息增益有更大的影响。但是如果直接将所有梯度较小的数据都丢弃掉势必会影响数据的总体分布。
            + 所以GOSS在选择梯度较大数据的同时随机选择了部分梯度较小的数据，这样算法就会更关注训练不足的样本，而不会过多改变原数据集的分布。

         6. 互斥特征捆绑算法EFB

            + 高维数据往往是稀疏的，因此可以考虑捆绑一部分特征来减少特征的维度。通常被捆绑的特征都是互斥的，因为这样才能不会丢失信息。
            + 可以用一个指标对特征不互斥程度进行衡量，称之为冲突比率，当这个值较小时，EFB算法选择把不完全互斥的两个特征捆绑，而不影响最后的精度。 这样不仅能够降低特征数量，也能够很好地降低构建直方图时的时间复杂度。

         7. 支持类别型特征处理（不包括字符串类型）

            + LightGBM采用 many-vs-many 的切分方式将类别特征分为两个子集，实现类别特征的最优切分。
            + 在枚举分割点之前，先把直方图按照每个类别对应的label均值进行排序；然后按照排序的结果依次枚举最优分割点。
            + 这个方法很容易过拟合，所以LightGBM里面还增加了很多对于这个方法的约束和正则化。

         8. 支持特征并行、数据并行、投票并行，应用于海量数据集。

      3. 优缺点

         优点

         + 采用直方图算法遍历样本，降低了时间复杂度以及内存消耗。
         + 训练过程中采用单边梯度算法过滤掉部分梯度小的样本，减少了大量的计算。
         + 采用了基于 Leaf-wise 算法的增长策略构建树，减少了很多不必要的计算量同时也减低了偏差。
         + 采用优化后的特征并行、数据并行方法加速计算，当数据量非常大的时候还可以采用投票并行的策略。
         + 采用互斥特征捆绑算法减少了特征数量，降低了内存消耗。

         缺点

         + 可能会长出比较深的决策树，产生过拟合。因此LightGBM在Leaf-wise之上增加了一个最大深度限制，在保证高效率的同时防止过拟合。
         + Boosting族是迭代算法，每一次迭代都根据上一次迭代的预测结果对样本进行权重调整，所以随着迭代不断进行，误差会越来越小，模型的偏差（bias）会不断降低。由于LightGBM是基于偏差的算法，所以会对噪点较为敏感。
         + 在寻找最优解时，依据的是最优切分变量，没有将最优解是全部特征的综合这一理念考虑进去。

  ​ 

## （四）模型效果评估

1. 预测问题评估方法

   1. 均方误差MSE

      + 参数估计值与真实值差的平方的期望
      + MSE越小，模型越精确

   2. 均方根误差RMSE

      MSE的算术平方根

   3. 平均绝对误差MAE

      绝对误差的平均值

2. 分类问题的评估方法

   TP（预测为正，实际为正），FN（预测为负，实际为正）

   FP（预测为正，实际为负），TN（预测为负，实际为负）

   1. 准确率、召回率、正确率

      + 准确率
        $$
        \frac{TP}{TP+FP}
        $$

      + 召回率
        $$
        \frac{TP}{TP+FN}
        $$

      + 正确率
        $$
        \frac{TP + TN}{TP+FN+FP+TN}
        $$
        

   2. 正确率与准确率的区别

      + 相比准确率，正确率同时考虑了正负样本的预测情况
      + 在实际问题中更关注正样本，并且会存在正负样本不平衡的概率（如正：负=1:999），所以准确率应用更广泛

   3. PR曲线

      + 用来可视化模型准确率和召回率
      + 在实际工作中，通常固定召回率，在此基础上提升准确率
      + 准确率为纵轴，查全率为横轴
      + 如果A模型的PR曲线被B模型的曲线完全包住，则B优于A
      + 如果A、B曲线有交叉，比较平衡点（BEP，PR曲线与y=x直线的交点）的大小，BEP越大，模型效果越好

   4. ROC曲线

      1. 横坐标FPR（假正例率/假阳率）、纵坐标TPR（真正例率/召回率）
      $$
      TPR = \frac{TP}{TP + FN}\\
      FPR = \frac{FP}{TN + FP}
      $$
   
      2. 绘制方法
         + 第一种是比较常用的方法，在二分类问题中，模型的输出一般是预测样本为正例概率，将样本按照预测概率从高到低排序。此时，我们需要指定一个阈值，当预测概率大于该阈值的样本会被判断为正例，小于该阈值的样本会被判断为负例，然后动态地调整这个阈值，从最高得分（实际上是从正无穷开始调整，对应roc曲线的（0,0）点坐标）开始调整，逐渐调整到最低得分，每个阈值都会对应着一个FPR（假阳率）和TPR（召回率），在roc图上绘制每个阈值所对应的位置，再连接所有点就得到了最终的roc曲线。
         + 第二种是更直观地绘制roc曲线的方法。首先根据样本标签统计出正负样本的数量，假设正样本数量为p，负样本数量为n；接下来，把横轴的刻度间隔设置为1/n,把纵轴的刻度设置为1/p；再根据模型输出的预测概率对样本进行从高到低的排序；依次遍历所有样本，同时从零点开始绘制roc曲线，每遇到一个正样本就沿着纵轴方向绘制一个刻度间隔的曲线，每遇到一个负样本就沿着横轴方向绘制一个刻度的曲线，直到遍历完所有样本，曲线最终停止在（1，1）坐标点，整个roc曲线绘制完成。
   + AUC
     
     ROC曲线下的面积，AUC越大，模型效果越好
   
3. 多分类问题的评估方法

   1. 将多分类转为二分类，选择最关心的分类为正例
   2. 使用混淆矩阵，将2*2预测值与实际结果之间对应的矩阵进行扩展，扩展成n * n（n为分类的数量），其中对角线代表正确的结果
   3. 如果关心的是模型整体的分类效果，采用正确率来描述

## （五）评分卡模型

1. 简介

   1. 定义

      + 以分数的形式来衡量风险几率的一种手段
      + 对未来一段时间内违约/逾期/失联概率的预测
      + 通常评分越高越安全
      + 根据使用场景分为申请评分卡 A卡、行为评分卡 B卡、催收评分卡 C卡、反欺诈评分卡 F卡（F卡不常用）

   2. 目的

      通过对借款人信用历史记录和业务活动记录的深度数据挖掘、分析和提炼，发现蕴藏在纷繁复杂数据中、反映消费者风险特征和预期信贷表现的知识和规律，并通过评分的方式总结出来，作为管理决策的科学依据。

   3. 意义

      相比于人工审批更加的客观、一致、准确，同时能够量化用户的风险程度，并且很大程度地提高审批效率（评分卡是在系统中自动实施的）。

   4. 类型

      1. 申请评分卡（A卡）
         + 属于贷前阶段
         + 目的在于预测申请时（申请信用卡、申请贷款）对申请人进行量化评估。
      2. 行为评分卡（B卡）
         + 属于贷后管理阶段
         + 目的在于预测使用时点（获得贷款、信用卡的使用期间）未来一定时间内逾期的概率。
      3. 催收评分卡（C卡）
         + 属于催收阶段
         + 用于催收管理，在借款人当前状态为逾期的情况下，预测未来该笔贷款变为坏账的概率。
      4. 反欺诈评分卡（F卡）
         + 属于贷前阶段
         + 用于对业务中的新用户可能存在的欺诈行为进行预测。

   5. 构建过程

      1. 数据分析：基于实际业务场景理解数据内容，发现数据与研究问题的关系。
      2. 数据处理：对原始数据进行处理，包括不同数据源间的数据合并、数据规整化处理、缺失值处理等环节。
      3. 特征选择：利用特征选择方法，筛选出预测能力强的有效特征，合理降低特征总维度。
      4. 特征分箱：对特征自变量进行离散化分箱处理。
      5. WOE转换：特征分箱处理后，将变量进行WOE编码转换。
      6. 模型建立：结合样本数据建立模型及模型参数输出过程。
      7. 模型评估：利用评估指标对模型效果进行评价。
      8. 评分转换：将模型概率转换为直观评分以及生成评分卡。
      9. 验证上线：验证评分卡效果，并上线持续监测。

   6. 优缺点

      优点

      + 易于使用。业务人员在操作时，只需要按照评分卡每样打分然后算个总分就能操作，不需要接受太多专业训练。
      + 直观透明。客户和审核人员都能知道看到结果，以及结果是如何产生的。
      + 应用范围广。芝麻信用分

      

      缺点

      + 在日益增长的数据前，有着大量数据资源却使用有限，造成数据资源的浪费。
      + 当信息维度高时，评分卡建模会变得非常困难。
      + 某些特征在不同时期的重要性不同，而评分卡并没有关注到这些。例如在疫情期间，和收入相关的特征重要度会上升。

   7. 选择逻辑回归而不是XGBOOST或神经网络的原因

      + 模型直观，可解释性强，易于理解，变量系数可以与业内知识做交叉验证，更容易让人信服。
      + 易于发现问题。当模型效果衰减的时候，logistic模型能更好的查找原因。

2. 分箱

   1. 定义

      对特征变量进行区间划分或者对不同枚举值进行合并的过程，它可以降低特征的复杂度，提升变量可解释性。

   2. 功能

      1. 拆分
         + 对 “连续变量” 进行分段离散化，使它变成 “离散变量”。比如：年龄、月收入。
         + 分为等频拆分、等距拆分、信息熵分箱。
      2. 合并
         + 减少离散变量的状态数，对 “离散变量” 进行合并。比如：城市、学历。

   3. 注意事项

      + 每个变量的分箱数，控制在十个以下，通常 5个左右是最佳的；
      + 每个箱子里的样本数至少为总样本数的5%以上，每个箱子里都要有正负样本，同时坏样本率与箱子呈单调关系。
      + 分箱越多，模型过拟合的风险越高，模型的稳定性也会变差，在金融场景，风险可控与稳定至关重要。

   4. 优缺点

      优点

      + 会 “降低特征变量的复杂度，降低模型过拟合的风险”（因为特征中值的数量变少了，每个取值的样本量增多，也就是说方差会相比于分箱前变小，而偏差会变大）；
      + 可以 “增强模型的稳定性”，对特征变量的异常波动不会反应太大，也利于适应更广泛的客群；
      + 将特征变量划分为有限的分箱，可以 “增强模型的可解释性”；
      + 可以更自然地将 “缺失值作为单独的分箱”。

      

      缺点

      + 分箱后丢失了数据的细节信息，使模型对好坏样本的判别能力变弱了，模型的KS和AUC相比于分箱前会下降。

   5. 卡方分箱

      1. 原理

         + 它以卡方分布和卡方值为基础，判断某个因素是否会影响目标变量。例如，在检验性别是否会影响违约概率时，可以用卡方检验来判断。

         + 卡方检验的无效假设H0是：观察频数与期望频数没有差别，即该因素不会影响到目标变量。
         + 基于该假设计算出χ2值，它表示观察值与理论值之间的偏离程度。根据χ2分布及自由度可以确定在H0假设成立的情况下获得当前统计量及更极端情况的概率P。
         + 如果P值很小，说明观察值与理论值偏离程度太大，应当拒绝无效假设，表示比较资料之间有显著差异；否则就不能拒绝无效假设，尚不能认为样本所代表的实际情况和理论假设有差别。

      2. 步骤

         + 将数值变量A排序后分成区间较多的若干组，设为A_1,A_2,…,A_n（如果变量取值超过一百种可以采用等距的方法划分100箱）
         + 计算A_1 与A_2合并后的卡方值，A_2 与A_3合并后的卡方值，直至A_(n-1) 与A_n合并后的卡方值
         + 找出上一步所有合并后的卡方值中最小的一个，假设为A_(i-1) 与A_i,将其合并形成新的A_(i-1)
         + 不断重复2和3，直至满足终止条件

      3. 终止条件

         + 某次合并后，最小的卡方值的p值超过0.9（或0.95，0.99等）[卡方值越大，P值越小，相关性越强，对Y的解释性越好]
         + 某侧合并后，总的未合并的区间数达到指定的数目（例如5，10，15等）

3. 相关指标

   1. KS值

      1. 定义

         + KS指标倾向于从概率角度衡量正负样本分布之间的差异。
         + 正是因为正负样本之间的模糊性和连续性，所以KS也是一条连续曲线。但最终为什么取一个最大值，主要原因是提取KS曲线中的一个显著特征，从而便于相互比较。
         + 召回率与假阳率的差值的最大值为特征的KS值，也就是累计正样本率大于累计负样本率的最大值，因为我们想抓到更多的坏人，也就是提高召回率，减少抓错的好人，减低假阳率。

      2. 取值范围

         + KS值范围为（0-1），一般选取0.2-0.75的特征，>0.75说明特征可能不可靠或者有误。

      3. 优点

         KS值不仅能够评价模型的区分能力，也能够评价单个特征的区分能力，前提是特征是有序且是线性的。

      4. 计算步骤

         + 计算每个评分区间的好坏账户数。
         + 计算每个评分区间的累计好账户数占总好账户数比率(good%)和累计坏账户数占总坏账户数比率(bad%)。
         + 计算每个评分区间累计坏账户占比与累计好账户占比差的绝对值（累计good%-累计bad%），然后对这些绝对值取最大值即得此评分卡的KS值。

      5. KS曲线

         KS曲线横坐标是评分区间或者是数据集百分比，纵坐标是假阳率和召回率，特征的ks=max|TPR-FPR|，对应下图红色曲线的最高点。其中召回率曲线（绿色曲线）在假阳率曲线（紫色曲线）上方。

   2. PSI值

      1. 定义

         PSI （Population Stability Index）称为群体稳定性指标，用来 “对比2个数据集的分布，是否发生比较大的偏差”，对比一定要有参照物，对评分卡模型来说，参照物是模型训练时的 “训练样本” (期望分布)，而评估对象称为 “验证样本”(实际分布)。
         $$
         psi = \sum_{i=1}^{n}{(A_i-E_i)*ln{(A_i/E_i)}}
         $$
         其中，A_i为实际分布，E_i为期望分布

      2. 取值

         PSI 可从两个计算维度来看，即评分 PSI和特征变量 PSI。PSI越小代表模型越稳定

         + 一般认为PSI小于0.1时候模型稳定性很高

         + 0.1-0.25代表模型良好，需要检测变化的原因
         + 大于0.25代表模型稳定性差，需要进一步分析特征变量，或者迭代模型。

      3. 应用场景

         + 可以对 “入模的每个特征变量” 进行分箱，在验证集与训练集上做PSI对比，判断样本是否发生大的变化。
         + PSI也可以做跨期验证。确保 “评分值、每个特征变量”，在近N个月的验证集上，对比训练集计算出的PSI，在可接受的范围内。

   3. WOE与IV值

      1. 定义

         WOE 

         组内坏样本数量与总坏样本数量的比值与组内好样本数数量与总好样本数量的比值的差异，这个差异是两个比值的比值。
         $$
         WOE = ln{\frac{( 坏样本 / 总坏样本 )}{( 好样本 / 总好样本 )}}\\=ln{\frac{( 坏样本 / 好样本 )}{( 总坏样本 / 总好样本 )}}
         $$
         IV 

         IV值是woe值的加权求和。主要是消除各分组中数量差异带来的误差（消除分组样本分布不均匀带来的影响）。
         $$
         IV_i = [\frac{坏样本}{总坏样本}-\frac{好样本}{总好样本}]*WOE\\
         IV = \sum_{i=1}^{n}{IV_i}
         $$

      2. 取值

         + 组内响应比例越高（负样本占组内样本比例）WOE越大，所以要消除分组数量差异带来的影响。
         + 当组内无负样本的时候，woe=负无穷、iv对应正无穷。
         + 当组内无正样本数是woe对应正无穷、iv对应负无穷，不管iv等于正无穷还是负无穷都是无意义的。
         + 这也是IV的一个缺点，不能自动处理变量的分组中出现响应比例为0或者100%的情况
         + IV值小于0.02代表无预测能力 一般0.02-0.1低，0.1-0.3中，0.3-0.5高，大于等于0.5代表可疑 一般我们选取0.2到0.5的特征，也就是预测能力中到预测能力高的特征。

      3. 为什么进行WOE编码

         因为WOE表示的是当前分箱中好坏客户的比例与总体好坏客户比例的差异，WOE的绝对值越大，这种差异就越明显，契合风控场景。

      4. 优缺点

         优点

         + 通过WOE编码可以对自变量X进行转换，使编码后的变量与因变量Y呈单调关系（是因为WOE编码后再通过逻辑回归拟合可以很好地契合评分卡公式）。
         + 对于稀疏变量，可以通过分箱将稀疏的值聚集到较大的组别，并最终可以使用 WOE 来表示整个组别的信息。提高模型的解释性。
         + 编码后特征的取值变少，在训练模型时能更快的收敛，提高了训练速度。
         + woe化之后可以计算每个特征的IV值，用来筛选特征。

         缺点

         + 由于最终分箱数量较少，会导致一定的信息丢失。
         + 未考虑自变量之间的相关性，即没有特征交互。
         + 计算过程利用了Y标签信息，因此存在特征穿越风险。

         特征穿越

         也可以叫做数据泄露，由于样本划分的策略，导致测试集中的信息引入到了训练集中，导致评估结果会更偏爱过拟合的模型，从而导致评估结果不够准确。

